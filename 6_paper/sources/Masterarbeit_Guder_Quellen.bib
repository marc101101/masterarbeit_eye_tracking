@article{NLapidot-Lefler2012,
author = {{N Lapidot-Lefler}, A Barak},
title = {{Effects of anonymity, invisibility, and lack of eye-contact on toxic online disinhibition}},
url = {http://www.columbia.edu/{~}rmk7/HC/HC{\_}Readings/Argyle.pdf},
year = {2012}
}
@misc{ITWissen2011,
author = {{IT Wissen}},
title = {{VNC (virtual network computing) :: ITWissen.info}},
url = {https://www.itwissen.info/VNC-virtual-network-computing.html},
urldate = {2019-09-24},
year = {2011}
}
@misc{StephenMatthews2016,
author = {{Stephen Matthews}},
title = {{Netherlands men top height table at just under 6ft while Guatemalan women are shortest | Daily Mail Online}},
url = {https://www.dailymail.co.uk/health/article-3706923/How-tall-country-Men-Netherlands-table-just-6ft-women-Guatemala-stand-4-10.html},
urldate = {2019-09-23},
year = {2016}
}
@misc{MultiCompLab2019,
author = {{MultiComp Lab}},
title = {{MultiComp | MultiComp Lab's mission is to build the algorithms and computational foundation to understand the interdependence between human verbal, visual, and vocal behaviors expressed during social communicative interactions.}},
url = {http://multicomp.cs.cmu.edu/},
urldate = {2019-09-21},
year = {2019}
}
@misc{PeterRobinson2016,
author = {{Peter Robinson}},
title = {{Computer Laboratory – Research in the Rainbow Group: Research}},
url = {https://www.cl.cam.ac.uk/research/rainbow/research/},
urldate = {2019-09-21},
year = {2016}
}
@misc{JanCarstenLohmuller,
author = {{Jan Carsten Lohm{\"{u}}ller}},
booktitle = {2016},
title = {{Reactive Programming – Mehr als nur Streams und Lambdas | Informatik Aktuell}},
url = {https://www.informatik-aktuell.de/entwicklung/programmiersprachen/reactive-programming-mehr-als-nur-streams-und-lambdas.html},
urldate = {2019-09-19}
}
@misc{Samadian2011,
author = {Samadian, Azam},
booktitle = {2017},
title = {{An Overview of Real-Time PCR Real-Time PCR}},
url = {https://deepstreamhub.com/blog/an-overview-of-realtime-protocols/},
urldate = {2019-09-19},
year = {2011}
}
@book{Buschmann2000,
abstract = {1. korr. Nachdr. Literaturverz. S. [433] - 443.},
author = {Buschmann, Frank.},
isbn = {3827312825},
publisher = {Addison-Wesley},
title = {{Pattern-orientierte Software-Architektur : ein Pattern-System}},
url = {https://books.google.de/books?hl=de{\&}lr={\&}id=o2nuK0qpo3QC{\&}oi=fnd{\&}pg=PR9{\&}dq=Pipeline+software+pattern{\&}ots=LbKEs-Dk1v{\&}sig=8ic8NFtX-V95Hwn1ld5vIgv0lZE{\#}v=onepage{\&}q=Pipeline software pattern{\&}f=false},
year = {2000}
}
@article{Saint-Andre2011,
abstract = {The WebSocket Protocol enables two-way communication between a client running untrusted code in a controlled environment to a remote host that has opted-in to communications from that code. The security model used for this is the origin-based security model commonly used by web browsers. The protocol consists of an opening handshake followed by basic message framing, layered over TCP. The goal of this technology is to provide a mechanism for browser-based applications that need two-way communication with servers that does not rely on opening multiple HTTP connections (e.g., using XMLHttpRequest or {\textless}iframe{\textgreater}s and long polling).},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Saint-Andre, Peter},
doi = {10.1017/CBO9781107415324.004},
eprint = {arXiv:1011.1669v3},
file = {:Users/markusguder/Desktop/Neuer Ordner/4{\_}Quellen/Melnikov - 2011 - RFC 6455 - The WebSocket Protocol.pdf:pdf},
isbn = {9788578110796},
issn = {1098-6596},
journal = {RFC 6455 (Proposed Standard)},
pages = {i --73},
pmid = {25246403},
title = {{RFC Standard 6455-- The WebSocket Protocol}},
url = {http://datatracker.ietf.org/doc/rfc6455/},
year = {2011}
}
@misc{Foundation2018,
author = {Foundation, Raspberry Pi},
booktitle = {Raspberry Pi Foundation},
title = {{Raspberry Pi Downloads - Software For the Raspberry Pi}},
url = {https://www.raspberrypi.org/downloads/},
urldate = {2019-09-19},
year = {2018}
}
@book{Reid1988,
abstract = {Mental workload is proposed to be a multidimensional construct that can be largely explained by three component factors: Time Load, Mental Effort Load, and Psychological Stress Load. In this paper, we describe a subjective scaling approach, the Subjective Workload Assessment Technique (SWAT), that captures this multidimensional nature of mental workload. We describe the SWAT procedure as a two-phased method that includes (a) a scale development phase based on conjoint measurement and nonmetric scaling, and (b) an event scoring phase. The development of SWAT and its measurement foundations are discussed. Recent research illustrating SWAT's widespread utility and its sensitivity as a measure of perceived mental workload is summarized.},
archivePrefix = {arXiv},
arxivId = {arXiv:1011.1669v3},
author = {Reid, Gary B. and Nygren, Thomas E.},
booktitle = {Advances in Psychology},
doi = {10.1016/S0166-4115(08)62387-0},
eprint = {arXiv:1011.1669v3},
isbn = {9780444703880},
issn = {01664115},
pages = {185--218},
pmid = {25246403},
publisher = {North-Holland},
title = {{Human Mental Workload}},
url = {http://www.sciencedirect.com/science/article/pii/S0166411508623870},
volume = {52},
year = {1988}
}
@article{Romaniuk2007,
abstract = {The credibility and vibrancy of any discipline depends on a willingness to question even the most strongly held beliefs. Our research challenges the central importance of differentiation to brand strategy. We provide an empirically grounded theoretical argument that differentiation plays a more limited role in brand competition than the orthodox literature assumes. We then present empirical data, spanning many categories and two countries, showing that there is a low level of perceived differentiation across competing brands. However, despite this lack of perceived differentiation, customers are still buying these brands. This leads us to question the importance of perceived and valued differentiation and to instead place distinctiveness at the centre of brand strategy - where a brand builds unique associations that simply make it more easily identifiable. We discuss the very positive implications for marketing management and call for research on being distinctive and getting noticed. {\textcopyright} 2007 Australian and New Zealand Marketing Academy.},
author = {Romaniuk, Jenni and Sharp, Byron and Ehrenberg, Andrew},
doi = {10.1016/S1441-3582(07)70042-3},
issn = {14413582},
journal = {Australasian Marketing Journal},
keywords = {Brand differentiation,Consumer perceptions,Distinctiveness,Perceived differentiation},
month = {jan},
number = {2},
pages = {42--54},
title = {{Evidence concerning the importance of perceived brand differentiation}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S1441358207700423},
volume = {15},
year = {2007}
}
@incollection{Hart1988,
abstract = {The results of a multi-year research program to identify the factors associated with variations in subjective workload within and between different types of tasks are reviewed. Subjective evaluations of 10 workload-related factors were obtained from 16 different experiments. The experimental tasks included simple cognitive and manual control tasks, complex laboratory and supervisory control tasks, and aircraft simulation. Task-, behavior-, and subject-related correlates of subjective workload experiences varied as a function of difficulty manipulations within experiments, different sources of workload between experiments, and individual differences in workload definition. A multi-dimensional rating scale is proposed in which information about the magnitude and sources of six workload-related factors are combined to derive a sensitive and reliable estimate of workload. {\textcopyright} 1988 Elsevier Science {\&} Technology.},
author = {Hart, Sandra G. and Staveland, Lowell E.},
booktitle = {Advances in Psychology},
doi = {10.1016/S0166-4115(08)62386-9},
issn = {01664115},
number = {C},
pages = {139--183},
title = {{Development of NASA-TLX (Task Load Index): Results of Empirical and Theoretical Research}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0166411508623869},
volume = {52},
year = {1988}
}
@techreport{Baltrusaitis2016,
abstract = {{\textcopyright} 2016 IEEE. Over the past few years, there has been an increased interest in automatic facial behavior analysis and understanding. We present OpenFace - an open source tool intended for computer vision and machine learning researchers, affective computing community and people interested in building interactive applications based on facial behavior analysis. OpenFace is the first open source tool capable of facial landmark detection, head pose estimation, facial action unit recognition, and eye-gaze estimation. The computer vision algorithms which represent the core of OpenFace demonstrate state-of-the-art results in all of the above mentioned tasks. Furthermore, our tool is capable of real-time performance and is able to run from a simple webcam without any specialist hardware. Finally, OpenFace allows for easy integration with other applications and devices through a lightweight messaging system.},
author = {Baltrusaitis, Tadas and Robinson, Peter and Morency, Louis Philippe},
booktitle = {2016 IEEE Winter Conference on Applications of Computer Vision, WACV 2016},
doi = {10.1109/WACV.2016.7477553},
file = {:Users/markusguder/Desktop/Neuer Ordner/4{\_}Quellen/Baltrusaitis, Robinson, Morency - 2016 - OpenFace An open source facial behavior analysis toolkit.pdf:pdf},
isbn = {9781509006410},
title = {{OpenFace: An open source facial behavior analysis toolkit}},
url = {https://www.omron.com/ecb/products/mobile/},
year = {2016}
}
@techreport{Maglio2000,
abstract = {{\textcopyright} Springer-Verlag Berlin Heidelberg 2000.The trend toward pervasive computing necessitates finding and implementing appropriate ways for users to interact with devices. We believe the future of interaction with pervasive devices lies in attentive user interfaces, systems that pay attention to what users do so that they can attend to what users need. Such systems track user behavior, model user interests, and anticipate user desires and actions. In addition to developing technologies that support attentive user interfaces, and applications or scenarios that use attentive user interfaces, there is the problem of evaluating the utility of the attentive approach. With this last point in mind, we observed users in an “office of the future”, where information is accessed on displays via verbal commands. Based on users' verbal data and eye-gaze patterns, our results suggest people naturally address individual devices rather than the office as a whole.},
author = {Maglio, Paul P. and Matlock, Teenie and Campbell, Christopher S. and Zhai, Shumin and Smith, Barton A.},
booktitle = {Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics)},
file = {:Users/markusguder/Desktop/Neuer Ordner/4{\_}Quellen/Maglio et al. - Unknown - Gaze and Speech in Attentive User Interfaces.pdf:pdf},
issn = {16113349},
pages = {1--7},
title = {{Gaze and speech in attentive user interfaces}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.28.8271{\&}rep=rep1{\&}type=pdf},
volume = {1948},
year = {2000}
}
@article{Ramaswamy2015,
abstract = {One of the buzzwords in the Information Technology is Internet of Things (IoT). The future is In-ternet of Things, which will transform the real world objects into intelligent virtual objects. The IoT aims to unify everything in our world under a common infrastructure, giving us not only con-trol of things around us, but also keeping us informed of the state of the things. In Light of this, present study addresses IoT concepts through systematic review of scholarly research papers, corporate white papers, professional discussions with experts and online databases. Moreover this research article focuses on definitions, geneses, basic requirements, characteristics and aliases of Internet of Things. The main objective of this paper is to provide an overview of Internet of Things, architectures, and vital technologies and their usages in our daily life. However, this manuscript will give good comprehension for the new researchers, who want to do research in this field of Internet of Things (Technological GOD) and facilitate knowledge accumulation in effi-ciently.},
author = {Madakam, Somayya and Ramaswamy, R. and Tripathi, Siddharth},
doi = {10.4236/jcc.2015.35021},
file = {:Users/markusguder/Desktop/Neuer Ordner/4{\_}Quellen/Ramaswamy, Tripathi - 2015 - Internet of Things (IoT) A Literature Review.pdf:pdf},
issn = {2327-5219},
journal = {Journal of Computer and Communications},
keywords = {Actuators,Barcode,Bluetooth,EPC,IPv6,Internet of Things,IoT,NFC,RFID,Sensors,Wi-Fi,ZigBee},
number = {05},
pages = {164--173},
title = {{Internet of Things (IoT): A Literature Review}},
url = {http://www.scirp.org/journal/jcchttp://dx.doi.org/10.4236/jcc.2015.35021http://dx.doi.org/10.4236/jcc.2015.35021},
volume = {03},
year = {2015}
}
@article{JustMA1980,
author = {{Just MA} and {Carpenter PA}},
file = {:Users/markusguder/Desktop/Neuer Ordner/4{\_}Quellen/file.pdf:pdf},
issn = {0033-295X},
journal = {Psychological review},
number = {4},
pages = {329--54},
title = {{(ref'd, skimmed, eye-mind hypothesis) A theory of reading: from eye fixations to comprehension.}},
volume = {87},
year = {1980}
}
@article{Khamis2015,
abstract = {Falling hardware prices led to a widespread use of public displays. Common interaction techniques for such displays currently include touch, mid-air, or smartphone-based in- teraction. While these techniques are well understood from a technical perspective, several remaining challenges hin- der the uptake of interactive displays among passersby. In this paper we propose addressing major public display chal- lenges through gaze as a novel interaction modality. We discuss why gaze-based interaction can tackle these chal- lenges effectively and discuss how solutions can be techni- cally realized. Furthermore, we summarize state-of-the-art eye tracking techniques that show particular promise in the area of public displays},
author = {Khamis, Mohamed and Bulling, Andreas and Alt, Florian},
doi = {10.1145/2800835.2807951},
file = {:Users/markusguder/Desktop/Neuer Ordner/4{\_}Quellen/Khamis, Bulling, Alt - 2015 - Tackling Challenges of Interactive Public Displays using Gaze.pdf:pdf},
isbn = {9781450335751},
journal = {UbiComp and ISWC 2015 - Proceedings of the 2015 ACM International Joint Conference on Pervasive and Ubiquitous Computing and the Proceedings of the 2015 ACM International Symposium on Wearable Computers},
keywords = {Digital signage,Gaze,Gaze-based interaction,Pervasive displays,Public displays},
pages = {763--766},
title = {{Tackling challenges of interactive public displays using gaze}},
url = {http://dx.doi.org/10.1145/2800835.2807951},
year = {2015}
}
@article{Zhang2014,
abstract = {Copyright {\textcopyright} 2014 by the Association for Computing Machinery, Inc. (ACM). Public displays can be made interactive by adding gaze control. However, gaze interfaces do not offer any physical affordance, and require users to move into a tracking range. We present GazeHorizon, a system that provides interactive assistance to enable passers-by to walk up to a display and to navigate content using their eyes only. The system was developed through field studies culminating in a four-day deployment in a public environment. Our results show that novice users can be facilitated to successfully use gaze control by making them aware of the interface at first glance and guiding them interactively into the tracking range.},
author = {Zhang, Yanxia and M{\"{u}}ller, J{\"{o}}rg and Chong, Ming Ki and Bulling, Andreas and Gellersen, Hans},
doi = {10.1145/2632048.2636071},
file = {:Users/markusguder/Desktop/Neuer Ordner/4{\_}Quellen/Zhang et al. - 2014 - GazeHorizon Enabling Passers-by to Interact with Public Displays by Gaze.pdf:pdf},
isbn = {9781450329682},
journal = {UbiComp 2014 - Proceedings of the 2014 ACM International Joint Conference on Pervasive and Ubiquitous Computing},
keywords = {Eye tracking,Field study,Gaze interface,Public display,Spontaneous interaction,Walk-up-and-use},
pages = {559--563},
title = {{GazeHorizon: Enabling passers-by to interact with public displays by gaze}},
url = {http://dx.doi.org/10.1145/2632048.2636071},
year = {2014}
}
@misc{Group2019,
author = {Group, Perceptual User Interfaces},
title = {{Prof. Dr. Andreas Bulling – Perceptual User Interfaces}},
url = {https://perceptual.mpi-inf.mpg.de/people/andreas-bulling/},
urldate = {2019-09-19},
year = {2019}
}
@article{Khamis2018,
abstract = {Gaze interaction holds a lot of promise for seamless human-computer interaction. At the same time, current wearable mobile eye trackers require user augmentation that negatively impacts natural user behavior while remote trackers require users to position themselves within a confined tracking range. We present GazeDrone, the first system that combines a camera-equipped aerial drone with a computational method to detect sidelong glances for spontaneous (calibration-free) gaze-based interaction with surrounding pervasive systems (e.g., public displays). GazeDrone does not require augmenting each user with on-body sensors and allows interaction from arbitrary positions, even while moving. We demonstrate that drone-supported gaze interaction is feasible and accurate for certain movement types. It is well-perceived by users, in particular while interacting from a fixed position as well as while moving orthogonally or diagonally to a display. We present design implications and discuss opportunities and challenges for drone-supported gaze interaction in public.

},
author = {Khamis, Mohamed and Kienle, Anna and Alt, Florian and Bulling, Andreas},
doi = {10.1145/3213526.3213539},
file = {:Users/markusguder/Desktop/Neuer Ordner/4{\_}Quellen/Khamis et al. - 2018 - GazeDrone Mobile Eye-Based Interaction in Public Space Without Augmenting the User.pdf:pdf},
isbn = {9781450358392},
journal = {DroNet 2018 - Proceedings of the 2018 ACM International Conference on Mobile Systems, Applications and Services},
keywords = {Active Eye Tracking,Drones,Gaze Interaction,UAV},
pages = {66--71},
title = {{GazeDrone: Mobile eye-based interaction in public space without augmenting the user}},
url = {http://eprints.gla.ac.uk/170214/1/170214.pdf},
year = {2018}
}
@inproceedings{ChanChaoNing2007,
abstract = {In this paper we describe an active gaze-tracking system that can provide an interactive interface with computer by using PTZ cameras. We propose an active tracking architecture for real-time gaze-tracking in an open environment and recorded the data into the database for diagnosis. When the cursor is out of control by gazing, a technical improvement theory is used to account for the reflective action of psychological natural reaction to correct the relevant process automatically. This architecture consists of four following parts: face tracking module, eye tracking module, active control module and input control module. We present a high accuracy gaze tracking system which can dynamically estimate the better target position of four PTZ and map the gazing point to screen coordinates. A user can use the active gaze-tracking system without difficult calibration and restricted posture. {\textcopyright} ICROS.},
author = {Ning, Chan Chao and Shunichiro, Oe and Lin, Chern Sheng},
booktitle = {ICCAS 2007 - International Conference on Control, Automation and Systems},
doi = {10.1109/ICCAS.2007.4406547},
isbn = {8995003871},
keywords = {Active vision,Automatic calibration,Eye-tracking,Gaze-tracking,Unrestricted posture tracking,Usability},
pages = {1348--1353},
publisher = {IEEE},
title = {{Development of an active gaze tracking system in unrestricted posture}},
url = {http://ieeexplore.ieee.org/document/4406547/},
year = {2007}
}
@inproceedings{Hennessey2012,
abstract = {The demand for improved human computer interaction will lead to increasing adoption of eye tracking in everyday devices. For interaction with devices such as Smart TVs, the eye tracker must operate in more challenging environments such as the home living room. In this paper we present a non-contact eye tracking system that allows for freedom of viewer motion in a living room environment. A pan and tilt mechanism is used to orient the eye tracker, guided by face tracking information from a wide-angle camera. The estimated point of gaze is corrected for viewer movement in realtime, avoiding the need for recalibration. The proposed technique achieves comparable accuracy to desktop systems near the calibration position of less than 1° of visual angle and accuracy of less than 2° of visual angle when the viewer moved a large distance, such as standing or sitting on the other side of the couch. The system performance achieved was more than sufficient to operate a novel, hands-free Smart TV interface.},
address = {New York, New York, USA},
author = {Iset, Jacob},
booktitle = {Eye Tracking Research and Applications Symposium (ETRA)},
doi = {10.1145/2168556.2168608},
isbn = {9781450312257},
keywords = {HCI,Smart TV,eye gaze tracking,interaction,living room,long range,pan and tilt},
pages = {249--252},
publisher = {ACM Press},
title = {{Long range eye tracking: Bringing eye tracking into the living room}},
url = {http://dl.acm.org/citation.cfm?doid=2168556.2168608},
year = {2012}
}
@article{Solomon2009,
abstract = {Purpose: To analyze the patient reported outcome of satisfaction after LASIK surgery. Design: Systematic review. Participants: Patient data from previously reported studies. Methods: A literature search conducted for the years 1988 to 2008 that included pertinent LASIK surgery information from the review of 2915 retrieved citations. All abstracts from these citations were reviewed and 1581 were deemed to be relevant for review. Complete copies of each of these relevant (1581) articles were obtained, and after thorough analysis each was rated based on the strength of the study design and weight of evidence. A level I rating was assigned to properly conducted, well-designed, randomized clinical trials; a level II rating to well-designed cohort and case-control studies; and a level III rating to case series, case reports, and poorly designed prospective and retrospective studies. Level I and II rated, peer-reviewed articles were entered into a database, and level III articles were eliminated. A total of 309 articles were incorporated into this database, representing level I and level II well-controlled studies of primary LASIK surgery. Main Outcome Measures: Patients' satisfaction rates and factors associated with dissatisfaction. Results: Nineteen of the 309 database articles (6.1{\%}) reported on both patient quality of life and satisfaction and together encompassed a total of 2198 subjects. The procedures from these 19 articles took place between 1995 and 2003. The overall patient satisfaction rate after primary LASIK surgery was 95.4{\%} (2097 of 2198 subjects; range of patient satisfaction for the 19 articles was 87.2{\%}-100{\%}). The patient satisfaction rate after myopic LASIK was 95.3{\%} (1811 of 1901 patients), and after hyperopic LASIK was 96.3{\%} (286 of 297 subjects). Conclusions: Based on this review, worldwide, an average 95.4{\%} of patients were satisfied with their outcome after LASIK surgery. With 16.3 million procedures performed worldwide, and more than a decade of clinical studies and technological innovation, LASIK surgery should be considered among the most successful elective procedures. LASIK surgery compares more favorably with other elective surgical procedures in terms of generally higher satisfaction rates. Financial Disclosure(s): Proprietary or commercial disclosure may be found after the references. {\textcopyright} 2009 American Academy of Ophthalmology.},
author = {Solomon, Kerry D. and {Fern{\'{a}}ndez de Castro}, Luis E. and Sandoval, Helga P. and Biber, Joseph M. and Groat, Brian and Neff, Kristiana D. and Ying, Michelle S. and French, John W. and Donnenfeld, Eric D. and Lindstrom, Richard L.},
doi = {10.1016/j.ophtha.2008.12.037},
issn = {01616420},
journal = {Ophthalmology},
month = {apr},
number = {4},
pages = {691--701},
title = {{LASIK World Literature Review. Quality of Life and Patient Satisfaction}},
url = {https://linkinghub.elsevier.com/retrieve/pii/S0161642008013377},
volume = {116},
year = {2009}
}
@article{Mrochen2001,
abstract = {Purpose: To study the advantage of modern eye-tracking systems for photorefractive surgery. Setting: Department of Ophthalmology, University of Zurich, Zurich, Switzerland. Methods: Photorefractive surgery (photorefractive keratectomy and laser in situ keratomileusis) for myopia and myopic astigmatism was performed in 40 eyes with a commercially available medical excimer laser system. The eyes were selected retrospectively from a larger group of patients treated at 1 clinic. In 20 eyes, the ablation was centered on the entrance pupil using the active, video-based, eye-tracking system (sampling frequency 50 Hz) of the laser. During laser treatment in the nontracker group (20 eyes), the active eye-tracking system was switched off and centration was done manually by the surgeon. Preoperatively and 1 and 3 months after surgery, the patients had a standard ophthalmic examination as well as wavefront analysis by means of a custom-designed wavefront analyzer. Results: After surgery, the visual acuity was significantly better (P {\textless} .05) in patients treated with the eye tracker. The increase in coma-like (relative increase factor 0.4) and spherical aberrations (relative increase factor 1.1) was significantly smaller in these patients than in those in the nontracker group (spherical equivalents of 3.9 and 5.1, respectively; P {\textless} .05). The refractive outcome, however, was not significantly different in sphere and cylinder. Conclusion: The use of active eye tracking appeared to improve the optical and visual outcomes but did not affect the refractive outcome after photorefractive laser surgery. {\textcopyright} 2001 ASCRS and ESCRS.},
author = {Mrochen, Michael and Eldine, Mostafa Salah and Kaemmerer, Maik and Seiler, Theo and H{\"{u}}tz, Werner},
doi = {10.1016/S0886-3350(00)00884-1},
issn = {08863350},
journal = {Journal of Cataract and Refractive Surgery},
month = {jul},
number = {7},
pages = {1000--1006},
pmid = {11489567},
publisher = {Elsevier},
title = {{Improvement in photorefractive corneal laser surgery results using an active eye-tracking system}},
url = {http://www.ncbi.nlm.nih.gov/pubmed/11489567},
volume = {27},
year = {2001}
}
@article{Khamis2017,
abstract = {While gaze holds a lot of promise for hands-free interaction with public displays, remote eye trackers with their confined tracking box restrict users to a single stationary position in front of the display. We present EyeScout, an active eye tracking system that combines an eye tracker mounted on a rail system with a computational method to automatically detect and align the tracker with the user's lateral movement. EyeScout addresses key limitations of current gaze-enabled large public displays by offering two novel gaze-interaction modes for a single user: In "Walk then Interact" the user can walk up to an arbitrary position in front of the display and interact, while in "Walk and Interact" the user can interact even while on the move. We report on a user study that shows that EyeScout is well perceived by users, extends a public display's sweet spot into a sweet line, and reduces gaze interaction kick-off time to 3.5 seconds -- a 62{\%} improvement over state of the art solutions. We discuss sample applications that demonstrate how EyeScout can enable position and movement-independent gaze interaction with large public displays.},
author = {Khamis, Mohamed and Hoesl, Axel and Klimczak, Alexander and Reiss, Martin and Alt, Florian and Bulling, Andreas},
doi = {10.1145/3126594.3126630},
file = {:Users/markusguder/Desktop/Neuer Ordner/4{\_}Quellen/Khamis et al. - 2017 - EyeScout Active Eye Tracking for Position and Movement Independent Gaze Interaction with Large Public Displays.pdf:pdf},
isbn = {9781450349819},
journal = {UIST 2017 - Proceedings of the 30th Annual ACM Symposium on User Interface Software and Technology},
keywords = {Body tracking,Gaze estimation,Gaze-enabled displays},
pages = {155--166},
title = {{EyeScout: Active eye tracking for position and movement independent gaze interaction with large public displays}},
url = {http://eprints.gla.ac.uk/170219/http://eprints.gla.ac.uk},
year = {2017}
}
@techreport{Shell2003,
abstract = {While preparing food, cooks often have to manage many time sensitive processes. Because cookbooks require visual and physical attention to use, they may distract, rather than focus the cook on executing the recipe. The knowledge requirements of cooking, concurrent demands for attention and the sensitivity of recipes to proper procedure conspire to make cooking a stressful experience, particularly to novices. We present eyeCOOK, a multimodal attentive cookbook which allows users to communicate using eye-gaze and speech. eyeCOOK responds visually and/or verbally, promoting communication through natural human input channels without physically encumbering the user. Our goal is to improve productivity and user satisfaction without creating additional requirements for user attention. We describe how the user interacts with the eyeCOOK prototype and the role of this system in an Attentive Kitchen.$\backslash$n},
author = {Shell, Jeffrey S and Bradbury, Jeremy S and Knowles, Craig B and Dickie, Connor and Vertegaal, Roel},
booktitle = {Video Program of the International Conference on Ubiquitous Computing (UbiComp 2003)},
file = {:Users/markusguder/Desktop/Neuer Ordner/4{\_}Quellen/Shell et al. - 2003 - eyeCOOK A Gaze and Speech Enabled Attentive Cookbook.pdf:pdf},
title = {{{\{}eyeCOOK{\}}: A Gaze and Speech Enabled Attentive Cookbook}},
url = {https://s3.amazonaws.com/academia.edu.documents/3246720/Shell{\_}Bradbudy{\_}Knowles{\_}Dickie{\_}Vertegaal{\_}2003.pdf?response-content-disposition=inline{\%}3B filename{\%}3DEyeCOOK{\_}A{\_}Gaze{\_}and{\_}Speech{\_}Enabled{\_}Attent.pdf{\&}X-Amz-Algorithm=AWS4-HMAC-SHA256{\&}X-Amz-Credential=AKI},
year = {2003}
}
@article{Majaranta2014,
abstract = {Eye tracking has a long history in medical and psychological research as a tool for recording and studying human visual behavior. Real-time gaze-based text entry can also be a powerful means of communication and control for people with physical disabilities. Following recent technological advances and the advent of affordable eye trackers, there is a growing interest in pervasive attention-aware systems and interfaces that have the potential to revolutionize mainstream human-technology interaction. In this chapter, we provide an introduction to the state-of-the art in eye tracking technology and gaze estimation. We discuss challenges involved in using a perceptual organ, the eye, as an input modality. Examples of real life applications are reviewed, together with design solutions derived from research results. We also discuss how to match the user requirements and key features of different eye tracking systems to find the best system for each task and application.},
author = {Majaranta, P{\"{a}}ivi and Bulling, Andreas},
doi = {10.1007/978-1-4471-6392-3_3},
file = {:Users/markusguder/Desktop/Neuer Ordner/4{\_}Quellen/Majaranta, Bulling - 2014 - Eye Tracking and Eye-Based Human–Computer Interaction.pdf:pdf},
pages = {39--65},
title = {{Eye Tracking and Eye-Based Human–Computer Interaction}},
url = {https://perceptual.mpi-inf.mpg.de/files/2014/07/majaranta14{\_}apc.pdf},
year = {2014}
}
@techreport{Kaur2003,
abstract = {The relationship between gaze and speech is explored for the simple task of moving an object from one location to another on a computer screen. The subject moves a designated object from a group of objects to a new location on the screen by stating, "Move it there." Gaze and speech data are captured to determine if we can robustly predict the selected object and destination position. We have found that the source fixation closest to the desired object begins, with high probability, before the beginning of the word "Move". An analysis of all fixations before and after speech onset time shows that the fixation that best identifies the object to be moved occurs, on average, 630 milliseconds before speech onset with a range of 150 to 1200 milliseconds for individual subjects. The variance in these times for individuals is relatively small although the variance across subjects is large. Selecting a fixation closest to the onset of the word "Move" as the designator of the object to be moved gives a system accuracy close to 95{\%} for all subjects. Thus, although significant differences exist between subjects, we believe that the speech and gaze integration patterns can be modeled reliably for individual users and therefore be used to improve the performance of multimodal systems. Copyright 2003 ACM.},
author = {Kaur, Manpreet and Tremaine, Marilyn and Huang, Ning and Wilder, Joseph and Gacovski, Zoran and Flippo, Frans and Mantravadi, Chandra Sekhar},
booktitle = {ICMI'03: Fifth International Conference on Multimodal Interfaces},
file = {:Users/markusguder/Desktop/Neuer Ordner/4{\_}Quellen/Kaur et al. - 2003 - Where is {\&}quotit{\&}quot Event Synchronization in Gaze-Speech Input Systems.pdf:pdf},
isbn = {1581136218},
keywords = {Eye-Tracking,Gaze-Speech Co-occurrence,Multimodal Fusion,Multimodal Interfaces},
pages = {151--158},
title = {{Where is "it"? Event synchronization in gaze-speech input systems}},
url = {http://immi.inesc-id.pt/immi04/works/week6/p151-kaur.pdf},
year = {2003}
}
@techreport{Rakoczi2012,
abstract = {Csanyi, Gottfried [Hrsg.]; Reichl, Franz [Hrsg.]; Steiner, Andreas [Hrsg.]: Digitale Medien - Werkzeuge f{\"{u}}r exzellente Forschung und Lehre. M{\"{u}}nster u.a. : Waxmann 2012, S. 87-98. - (Medien in der Wissenschaft; 61) Pedagogical sub-discipline: Media Education;},
author = {Rakoczi, Gergely},
booktitle = {Digitale - Medien: Werkzeuge f{\"{u}}r exzellente Forschung und Lehre. M{\"{u}}nster u.a. : Waxmann 2012, S. 87-98. - (Medien in der Wissenschaft; 61)},
file = {:Users/markusguder/Desktop/Neuer Ordner/4{\_}Quellen/Csanyi, Gottfried Reichl, Franz Steiner - 2012 - Eye Tracking in Forschung und Lehre. M{\"{o}}glichkeiten und Grenzen eines vielversprechende.pdf:pdf},
isbn = {978-3-8309-2741-9; 3-8309-2741-X},
issn = {1434-3436},
keywords = {Analyseverfahren,Analysis procedure,Augenbewegung,Cognitive process,Computertechnologie,Eye movements,Informationsaufnahme,Kognitiver Prozess},
pages = {87--98},
title = {{Eye Tracking in Forschung und Lehre. M{\"{o}}glichkeiten und Grenzen eines vielversprechenden Erkenntnismittels}},
url = {https://www.pedocs.de/frontdoor.php?la=en{\&}source{\_}opus=8301},
year = {2012}
}
@book{AndrewT.Duchowski2007,
abstract = {Despite the availability of cheap, fast, accurate and usable eye trackers, there is still little information available on how to develop, implement and use these systems. This second edition of Andrew Duchowski's successful guide to these systems contains significant additional material on the topic and fills this gap in the market with this accessible and comprehensive introduction. Opening with useful background information, including an introduction to the human visual system and key issues in visual perception and eye movement, the second part surveys eye-tracking devices and provides a detailed introduction to the technical requirements necessary for installing a system and developing an application program. The book focuses on video-based, corneal-reflection eye trackers - the most widely available and affordable type of system, before closing with a look at a number of interesting and challenging applications in human factors, collaborative systems, virtual reality, marketing and advertising. {\textcopyright} Springer-Verlag London Limited 2007.},
address = {London},
author = {Duchowski, Andrew},
booktitle = {Eye Tracking Methodology: Theory and Practice},
doi = {10.1007/978-1-84628-609-4},
isbn = {9781846286087},
pages = {1--328},
publisher = {Springer London},
title = {{Eye tracking methodology: Theory and practice}},
url = {http://link.springer.com/10.1007/978-1-84628-609-4},
year = {2007}
}
@techreport{Møllenbach2019,
abstract = {Gaze, as a sole input modality must support complex navigation and selection tasks. Gaze interaction combines specific eye movements and graphic display objects (GDOs). This paper suggests a unifying taxonomy of gaze interaction principles. The taxonomy deals with three types of eye movements: fixations, saccades and smooth pursuits and three types of GDOs: static, dynamic, or absent. This taxonomy is qualified through related research and is the first main contribution of this paper. The second part of the paper offers an experimental exploration of single stroke gaze gestures (SSGG). The main findings suggest (1) that different lengths of SSGG can be used for interaction, (2) that GDOs are not necessary for successful completion, and (3) that SSGG are comparable to dwell time selection.},
author = {M{\o}llenbach, Emilie and Hansen, John Paulin and Lillholm, Martin},
booktitle = {Journal of Eye Movement Research},
file = {:Users/markusguder/Desktop/Neuer Ordner/4{\_}Quellen/M{\o}llenbach, Hansen, Lillholm - 2019 - Eye Movements in Gaze Interaction.pdf:pdf},
issn = {19958692},
keywords = {Eye movements,Gaze Interaction,Selection Strategies in Gaze Interaction},
number = {2},
pages = {1--1},
publisher = {APA},
title = {{Eye movements in gaze interaction}},
url = {https://orbit.dtu.dk/files/120332523/Eye{\_}Movements{\_}in{\_}Gaze{\_}Interaction.pdf},
volume = {6},
year = {2013}
}
@book{Kranz2007,
abstract = {We report on approaches for context-awareness in a kitchen environment. Two devices, an augmented cutting board and a sensor-enriched knife, enable the environment to determine the type of food handled during the preparation of meals. Copyright 2007 ACM.},
author = {Kranz, Matthias and Schmidt, Albrecht and Maldonado, Alexis and Rusu, Radu Bogdan and Beetz, Michael and H{\"{o}}rnler, Benedikt and Rigoll, Gerhard},
booktitle = {TEI'07: First International Conference on Tangible and Embedded Interaction},
doi = {10.1145/1226969.1227013},
file = {:Users/markusguder/Desktop/Neuer Ordner/4{\_}Quellen/Kranz et al. - 2007 - Context-aware kitchen utilities.pdf:pdf},
isbn = {159593619X},
pages = {213--214},
title = {{Context-aware kitchen utilities}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.92.8416{\&}rep=rep1{\&}type=pdf},
year = {2007}
}
@techreport{Chen2010,
abstract = {The paper discusses a kitchen that intelligently senses cooking activities and provides realtime nutritional information helps facilitate healthy cooking by letting family cooks make informed decisions. It creates opportunities to embed pervasive computing in a smart kitchen to facilitate healthy cooking.},
author = {Chen, Jen Hao and Chi, Peggy Pei Yu and Chu, Hao Hua and Chen, Cheryl Chia Hui and Huang, Polly},
booktitle = {IEEE Pervasive Computing},
doi = {10.1109/MPRV.2010.75},
file = {:Users/markusguder/Desktop/Neuer Ordner/4{\_}Quellen/Chen et al. - 2010 - A smart kitchen for nutrition-aware cooking.pdf:pdf},
isbn = {15361268/10},
issn = {15361268},
keywords = {healthy cooking,information display,persuasive computing},
number = {4},
pages = {58--65},
title = {{A smart kitchen for nutrition-aware cooking}},
url = {www.computer.org/pervasive},
volume = {9},
year = {2010}
}
@techreport{Brooks2003,
abstract = {For a computational device to be context aware, it needs to be sensitive to physical, social, and task situations. While answering the five basic narrative questions of Who, What, When, Where and Why provides a tremendous amount of understanding and context to a story, making a computational or communications device sensitive to these same questions provide simple and powerful structural guidelines for context awareness in device interface design.},
author = {BROOKS, K},
booktitle = {Human-Automation Interaction: Research {\&} Practice},
file = {:Users/markusguder/Desktop/Neuer Ordner/4{\_}Quellen/St - 2003 - The Context Quintet Narrative Elements Applied to Context Awareness 1 Kevin Brooks.pdf:pdf},
isbn = {0805849327},
keywords = {02-09-10: Language communication and comprehension,37-02-00: System adaptability and flexibility,63-10-04: Textual analysis and parsing},
pages = {1213--1217},
title = {{The Context Quintet: Narrative Elements Applied to Context Awareness}},
url = {http://ezproxy.net.ucf.edu/login?url=http://search.ebscohost.com/login.aspx?direct=true{\&}db=ega{\&}AN=ega198724{\&}site=ehost-live},
year = {2003}
}
@techreport{Augusto2007,
abstract = {The Fourier transform technique, originally developed for spatial fringe pattern analysis, has been applied to the analysis of a temporal fringe signal obtained by a wavelength-shift interferometer used for absolute distance measurements. It has been shown that the error caused by the nonlinear and time-varying current-wavelength characteristic of the laser diode can be removed by combining the Fourier transform technique with the reference technique. A novel technique for distance measurement based on multiple-beam interferometry has been proposed, and an experimental demonstration is given for a three-beam interferometer that includes a reference reflector as an integral part of the system. Error sources and the limitation of the technique are discussed.},
author = {Augusto, Juan and Mccullagh, Paul},
booktitle = {Computer Science and Information Systems},
doi = {10.2298/csis0701001a},
file = {:Users/markusguder/Desktop/Neuer Ordner/4{\_}Quellen/Augusto, Mccullagh - 2007 - Ambient Intelligence Concepts and Applications.pdf:pdf},
issn = {1820-0214},
number = {1},
pages = {1--27},
title = {{Ambient Intelligence: Concepts and applications}},
url = {https://pdfs.semanticscholar.org/2b64/fae8054ebffd3e41ebe0cf7eb6c70f4725f9.pdf},
volume = {4},
year = {2007}
}
@techreport{Kazmierzak,
abstract = {—Smart home environments are environments that try to facilitate the life of the user in many different ways and make it more comfortable by using technology. This paper deals with the realization of such an environment and which benefits may arise from. Also it discuess why such a development is desirable.},
author = {Kazmierzak, Florian},
booktitle = {SNET Project},
file = {:Users/markusguder/Desktop/Neuer Ordner/4{\_}Quellen/Kazmierzak - 2011 - Smart Home Environment-Concepts and Solutions.pdf:pdf},
title = {{Smart Home Environment-Concepts and Solutions}},
url = {https://pdfs.semanticscholar.org/69ef/06b2525ca4aa43499046261a5fe41ab03322.pdf{\%}0Ahttp://www.snet.tu-berlin.de/fileadmin/fg220/courses/WS1112/snet-project/smart-home-environments-kazmierzak.pdf},
year = {2011}
}
@techreport{Harper2003,
abstract = {Springer},
author = {Meadows-Klue, Danny},
booktitle = {Interactive Marketing},
doi = {10.1057/palgrave.im.4340249},
file = {:Users/markusguder/Desktop/Neuer Ordner/4{\_}Quellen/Meadows-Klue - 2004 - Inside the Smart Home.pdf:pdf},
issn = {1463-5178},
number = {3},
pages = {307--308},
title = {{Inside the Smart Home}},
url = {http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.661.3611{\&}rep=rep1{\&}type=pdf},
volume = {5},
year = {2004}
}
@incollection{Ludwig2015,
abstract = {Industrie 4.0 bietet f{\"{u}}r jedes Unternehmen individuelle Potenziale und Entwicklungs-m{\"{o}}glichkeiten. Kern von „Industrie 4.0“ ist die Realisierung einer sog. „Smart Facto-ry“. Diese zeichnet sich vor allem durch die intelligente Vernetzung von Menschen und Maschinen in Produktion und Logistik aus. In Konsequenz werden flexiblere, effizientere und kundenindividuellere Produktions-und Logistikprozesse erm{\"{o}}glicht (Kagermann et al. 2013). Zentrale Voraussetzung ist die digitale Verf{\"{u}}gbarkeit aller erforderlichen Informationen und deren Verarbeitung in Echtzeit mittels innovativer Informations-und Kommunikationstechnologien (IKT) (BMWI 2014). Zur zuk{\"{u}}nftigen Integration des Menschen in die Smart Factory sind neuartige Mensch-Maschine-Schnittstellen zwingend erforderlich. Diese sollen gestaltet werden durch interaktive Assistenzsysteme (Dombrowski/Wagner 2014; Spath/Ganschar 2013).Interaktive Assistenzsysteme nehmen Daten mittels Sensoren auf und wandeln diese in elektrische Signale um. Diese werden von integrierten Mikrocomputern zu Anwei-sungen oder Handlungsempfehlungen verarbeitet und Mitarbeitern in Produktion und Logistik bereitgestellt (Neugebauer 2014). Typische Beispiele f{\"{u}}r interaktive Assis-tenzsysteme im hier gemeinten Kontext sind: Head-Mounted Displays, Pick-by-Technologie und Datenbrillen.Dieses Kompendium richtet sich an die Gesch{\"{a}}ftsleitungsowie Produktionsleiterproduzierender, kleiner und mittelst{\"{a}}ndischer Unternehmen (KMU), die ihre individu-ellen Potenziale von Industrie 4.0 identifizieren und aussch{\"{o}}pfen m{\"{o}}chten. Der Fo-kus liegt dabei bei dem Aufbau einer smarten Produktion und Logistik mittels interak-tiver Assistenzsysteme.},
author = {Ludwig, Bernd},
booktitle = {Technologie-Kompendium Interaktive Assistenzsysteme Vorwort},
doi = {10.1007/978-3-662-44819-9},
isbn = {978-3-662-44818-2},
pages = {5--46},
title = {{Interaktive Assistenzsysteme}},
url = {http://link.springer.com/10.1007/978-3-662-44819-9},
year = {2015}
}
@article{Biundo2010,
abstract = {Der {\{}Sonderforschungsbereich/Transregio{\}} 62 {\{}„Eine{\}} Companion-Technologie f{\"{u}}r kognitive technische Systeme“ befasst sich mit der systematischen und interdisziplin{\"{a}}ren Erforschung kognitiver F{\"{a}}higkeiten und deren Realisierung in technischen Systemen. Dabei stehen die Eigenschaften der Individualit{\"{a}}t, Anpassungsf{\"{a}}higkeit, Verf{\"{u}}gbarkeit, Kooperativit{\"{a}}t und Vertrauensw{\"{u}}rdigkeit im Mittelpunkt der Untersuchung. Die Realisierung dieser so-genannten Companion-Eigenschaften soll menschlichen Nutzern eine neue – auch emotionale – Dimension des Umgangs mit interaktiven Systemen erschlie{\ss}en, wobei diese als empathische Assistenten wahrgenommen und akzeptiert werden.},
author = {Biundo, Susanne and Wendemuth, Andreas},
doi = {10.1007/s13218-010-0056-9},
issn = {0933-1875},
journal = {KI - K{\"{u}}nstliche Intelligenz},
month = {nov},
number = {4},
pages = {335--339},
publisher = {Springer-Verlag},
title = {{Von kognitiven technischen Systemen zu Companion-Systemen}},
url = {http://link.springer.com/10.1007/s13218-010-0056-9},
volume = {24},
year = {2010}
}
@book{Poslad2009,
abstract = {This book provides an introduction to the complex field of ubiquitous computing Ubiquitous Computing (also commonly referred to as Pervasive Computing) describes the ways in which current technological models, based upon three base designs: smart (mobile, wireless, service) devices, smart environments (of embedded system devices) and smart interaction (between devices), relate to and support a computing vision for a greater range of computer devices, used in a greater range of (human, ICT and physical) environments and activities. The author details the rich potential of ubiquitous computing, the challenges involved in making it a reality, and the prerequisite technological infrastructure. Additionally, the book discusses the application and convergence of several current major and future computing trends. Key Features: Provides an introduction to the complex field of ubiquitous computing Describes how current technology models based upon six different technology form factors which have varying degrees of mobility wireless connectivity and service volatility: tabs, pads, boards, dust, skins and clay, enable the vision of ubiquitous computing Describes and explores how the three core designs (smart devices, environments and interaction) based upon current technology models can be applied to, and can evolve to, support a vision of ubiquitous computing and computing for the future Covers the principles of the following current technology models, including mobile wireless networks, service-oriented computing, human computer interaction, artificial intelligence, context-awareness, autonomous systems, micro-electromechanical systems, sensors, embedded controllers and robots Covers a range of interactions, between two or more UbiCom devices, between devices and people (HCI), between devices and the physical world. Includes an accompanying website with PowerPoint slides, problems and solutions, exercises, bibliography and further reading Graduate students in computer science, electrical engineering and telecommunications courses will find this a fascinating and useful introduction to the subject. It will also be of interest to ICT professionals, software and network developers and others interested in future trends and models of computing and interaction over the next decades.},
author = {Poslad, Stefan},
booktitle = {Ubiquitous Computing: Smart Devices, Environments and Interactions},
doi = {10.1002/9780470779446},
file = {:Users/markusguder/Desktop/Neuer Ordner/4{\_}Quellen/Poslad - 2009 - Ubiquitous Computing Smart Devices, Environments and Interactions.pdf:pdf},
isbn = {9780470035603},
pages = {1--473},
title = {{Ubiquitous Computing: Smart Devices, Environments and Interactions}},
url = {http://www.eecs.qmul.ac.uk/{~}stefan/publications/2009-poslad-Ubiquitous Computing-chapter1.pdf},
year = {2009}
}
@book{Cook2005,
abstract = {{\textcopyright} 2005 by John Wiley  {\&}  Sons, Inc. All rights reserved. Smart Environments contains contributions from leading researchers, describing techniques and issues related to developing and living in intelligent environments. Reflecting the multidisciplinary nature of the design of smart environments, the topics covered include the latest research in smart environment philosophical and computational architecture considerations, network protocols for smart environments, intelligent sensor networks and powerline control of devices, and action prediction and identification.},
author = {Cook, Diane J. and Das, Sajal K.},
booktitle = {Smart Environments: Technology, Protocols and Applications},
doi = {10.1002/047168659X},
isbn = {9780471686590},
pages = {1--404},
publisher = {John Wiley},
title = {{Smart Environments: Technology, Protocols and Applications}},
url = {https://books.google.de/books?hl=de{\&}lr={\&}id=fZ5gfxMLw-oC{\&}oi=fnd{\&}pg=PR7{\&}dq=mart+Environments:+Technology,+Protocols+and+Applications.+Wiley-Interscience{\&}ots=OUi0FAQFj0{\&}sig=P9yw{\_}ouQRGtMdp7cOklfY03Yeic{\#}v=onepage{\&}q{\&}f=false},
year = {2005}
}
@misc{MarkWeiser,
abstract = {Presenteation of technology overview for ubiquitous / pervasive computing},
author = {{Mark Weiser}},
booktitle = {Scientific American},
number = {3},
pages = {94--104},
title = {{The Computer for the 21st Century}},
url = {http://www.cse.nd.edu/{~}cpoellab/teaching/cse40463/weiser.pdf},
urldate = {2019-09-19},
volume = {265},
year = {1991}
}
@misc{NicoHornig2017,
author = {{Nico Hornig}},
title = {{Die Probleme von Alexa, Siri und Google Assistant}},
url = {https://www.wiwo.de/unternehmen/it/die-probleme-von-alexa-siri-und-google-assistant-warum-spracherkennung-so-schwierig-ist/20327384.html},
urldate = {2019-09-19},
year = {2017}
}
@techreport{Gruber2014,
author = {Gruber, Thomas R and Hills, Emerald},
file = {:Users/markusguder/Desktop/Neuer Ordner/4{\_}Quellen/Gruber, Hills - 2014 - Voice trigger for a digital assistant.pdf:pdf},
title = {{Voice trigger for a digital assistant}},
url = {https://patentimages.storage.googleapis.com/7c/2d/8f/ea11a2c87cda3a/US20140222436A1.pdf},
year = {2014}
}
@misc{AXAVersicherung,
author = {{AXA Versicherung}},
title = {{Unf{\"{a}}lle zu Hause vermeiden: Sicherheits-Tipps ⁄ AXA}},
url = {https://www.axa.de/das-plus-von-axa/haus-mieten-wohnen/haus-unfall/unfaelle-im-haus-checkliste},
urldate = {2019-09-19},
year = {2019}
}
@article{Blasco2014,
abstract = {The kitchen environment is one of the scenarios in the home where users can benefit from Ambient Assisted Living (AAL) applications. Moreover, it is the place where old people suffer from most domestic injuries. This paper presents a novel design, implementation and assessment of a Smart Kitchen which provides Ambient Assisted Living services; a smart environment that increases elderly and disabled people's autonomy in their kitchen-related activities through context and user awareness, appropriate user interaction and artificial intelligence. It is based on a modular architecture which integrates a wide variety of home technology (household appliances, sensors, user interfaces, etc.) and associated communication standards and media (power line, radio frequency, infrared and cabled). Its software architecture is based on the Open Services Gateway initiative (OSGi), which allows building a complex system composed of small modules, each one providing the specific functionalities required, and can be easily scaled to meet our needs. The system has been evaluated by a large number of real users (63) and carers (31) in two living labs in Spain and UK. Results show a large potential of system functionalities combined with good usability and physical, sensory and cognitive accessibility},
author = {Blasco, Rub{\'{e}}n and Marco, {\'{A}}lvaro and Casas, Roberto and Cirujano, Diego and Picking, Richard},
doi = {10.3390/s140101629},
file = {:Users/markusguder/Desktop/Neuer Ordner/4{\_}Quellen/Blasco et al. - 2014 - A Smart Kitchen for Ambient Assisted Living.pdf:pdf},
issn = {14248220},
journal = {Sensors (Switzerland)},
keywords = {Ambient assisted living,Ambient intelligence,Context and user awareness,Distributed sensor networks,OSGi,Smart homes},
number = {1},
pages = {1629--1653},
title = {{A Smart Kitchen for Ambient Assisted Living}},
url = {www.mdpi.com/journal/sensorsArticle},
volume = {14},
year = {2014}
}
@article{Solaimani,
abstract = {{\textcopyright} The Author(s) 2013 Reprints and permissions: sagepub.co.uk/journalsPermissions.nav.Technological innovations, from ubiquitous computing, augmented reality, telecommunication to intelligent appliances and robotics, bring new possibilities to the Smart Home domain, which has led to an increase in the number of academic publications in this domain. To date, no comprehensive overview and clustering of the core concepts used in these publications have been produced. Based on an extensive review of existing literature on the Smart Home, this paper visualizes the state of the art in the Smart Home research in a systematic way and outlines future research challenges. To do so, a business model framework is applied that helps researchers place their work within a broader context and identify gaps in the existing body of knowledge in this area. In order to move from the exploration towards the exploitation of Smart Home concepts, it is essential to contribute to a coherent body of knowledge that not only is technology driven, as it is the case now, but also pay attention to the non-technological aspects, i.e. social-organizational, economical, organizational, law/legislation and entrepreneurial topics, from both a strategic and an operational perspective.},
author = {Solaimani, Sam and Keijzer-Broers, Wally and Bouwman, Harry},
doi = {10.1177/1420326X13516350},
file = {:Users/markusguder/Desktop/Neuer Ordner/4{\_}Quellen/Solaimani, Keijzer-Broers, Bouwman - 2015 - What we do - And don't - Know about the Smart Home An analysis of the Smart Home literature.pdf:pdf},
issn = {14230070},
journal = {Indoor and Built Environment},
keywords = {Literature review,Qualitative meta-analysis,Service-Technology-Organization-Finance model (STO,Smart Home},
number = {3},
pages = {370--383},
title = {{What we do - And don't - Know about the Smart Home: An analysis of the Smart Home literature}},
url = {https://s3.amazonaws.com/academia.edu.documents/37668348/Indoor{\_}and{\_}Built{\_}Environment-2015-Solaimani-370-83.pdf?response-content-disposition=inline{\%}3B filename{\%}3DWhat{\_}we{\_}do{\_}and{\_}don{\_}t{\_}know{\_}about{\_}the{\_}Smar.pdf{\&}X-Amz-Algorithm=AWS4-HMAC-SHA256{\&}X-Amz-Credentia},
volume = {24},
year = {2015}
}
@techreport{GFK2016,
author = {GFK},
file = {:Users/markusguder/Desktop/Neuer Ordner/4{\_}Quellen/GFK - 2016 - GfK Future of Smart Home Study.pdf:pdf},
number = {January},
pages = {70},
title = {{GfK Future of Smart Home Study}},
url = {https://www.gfk.com/fileadmin/user{\_}upload/dyna{\_}content/GB/documents/Innovation{\_}event/GfK{\_}Future{\_}of{\_}Smart{\_}Home{\_}{\_}Global{\_}.pdf},
year = {2016}
}
@techreport{XuLiRongxingLu2011,
abstract = {In this article, we introduce an Internet of Things application, smart community, which refers to a paradigmatic class of cyber-physical systems with cooperating objects (i.e., networked smart homes). We then define the smart community architecture, and describe how to realize secure and robust networking among individual homes. We present two smart community applications, Neighborhood Watch and Pervasive Healthcare, with supporting techniques and associated challenges, and envision a few valueadded smart community services.},
author = {Li, Xu and Lu, Rongxing and Liang, Xiaohui and Shen, Xuemin and Chen, Jiming and Lin, Xiaodong},
booktitle = {IEEE Communications Magazine},
doi = {10.1109/MCOM.2011.6069711},
file = {:Users/markusguder/Desktop/Neuer Ordner/4{\_}Quellen/Li et al. - 2011 - Smart community An internet of things application.pdf:pdf},
issn = {01636804},
number = {11},
pages = {68--75},
title = {{Smart community: An internet of things application}},
url = {www.homeplug.},
volume = {49},
year = {2011}
}
